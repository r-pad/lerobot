#!/usr/bin/env python

# Copyright 2024 Columbia Artificial Intelligence, Robotics Lab,
# and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Diffusion Policy as per "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"

TODO(alexander-soare):
  - Remove reliance on diffusers for DDPMScheduler and LR scheduler.
"""

import math
import json
from collections import deque
from typing import Callable
import os

import einops
import numpy as np
import torch
import torch.nn.functional as F  # noqa: N812
import torchvision
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from torch import Tensor, nn

from lerobot.common.constants import OBS_ENV, OBS_ROBOT
from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig
from lerobot.common.policies.normalize import Normalize, Unnormalize
from lerobot.common.policies.pretrained import PreTrainedPolicy
from lerobot.common.policies.utils import (
    get_device_from_parameters,
    get_dtype_from_parameters,
    get_output_shape,
    populate_queues,
)
from lerobot.common.policies.high_level.high_level_wrapper import HighLevelWrapper, HighLevelConfig, get_siglip_text_embedding, initialize_mimicplay_model, TARGET_SHAPE, rgb_preprocess, depth_preprocess, get_scaled_intrinsics
from transformers import AutoModel, AutoProcessor


def repeat_goal_first_channel_as_rgb(batch, goal_key_name):
    """Repeat the first channel of a goal image thrice to create an RGB-compatible image.

    This is useful when the high-level model produces a single-channel heatmap that needs
    to be used as input to an RGB encoder.

    Args:
        batch: Dictionary containing image tensors
        goal_key_name: Key name for the goal image tensor to transform

    Returns:
        batch: Updated batch dictionary with modified goal image
    """
    if len(batch[goal_key_name].shape) == 5: # train
        batch[goal_key_name] = batch[goal_key_name][:, :, 0:1].repeat(1, 1, 3, 1, 1)
    elif len(batch[goal_key_name].shape) == 4: # inference, no history
        batch[goal_key_name] = batch[goal_key_name][:, 0:1].repeat(1, 3, 1, 1)
    else:
        raise ValueError("unexpected tensor shape")
    return batch

class DiffusionPolicy(PreTrainedPolicy):
    """
    Diffusion Policy as per "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
    (paper: https://arxiv.org/abs/2303.04137, code: https://github.com/real-stanford/diffusion_policy).
    """

    config_class = DiffusionConfig
    name = "diffusion"

    def __init__(
        self,
        config: DiffusionConfig,
        dataset_stats: dict[str, dict[str, Tensor]] | None = None,
    ):
        """
        Args:
            config: Policy configuration class instance or None, in which case the default instantiation of
                the configuration class is used.
            dataset_stats: Dataset statistics to be used for normalization. If not passed here, it is expected
                that they will be passed with a call to `load_state_dict` before the policy is used.
        """
        super().__init__(config)
        config.validate_features()
        self.config = config

        self.robot_adapter = config.get_robot_adapter()
        self.obs_key = self.robot_adapter.get_obs_key()
        self.act_key = self.robot_adapter.get_act_key()

        self.normalize_inputs = Normalize(config.input_features, config.normalization_mapping, dataset_stats)
        self.normalize_targets = Normalize(
            config.output_features, config.normalization_mapping, dataset_stats
        )
        self.unnormalize_outputs = Unnormalize(
            config.output_features, config.normalization_mapping, dataset_stats
        )

        # queues are populated during rollout of the policy, they contain the n latest observations and actions
        self._queues = None
        self.latest_gripper_proj = {}
        # For relative actions: save reference EEF pose for the current action chunk
        self._relative_action_reference_eef = None

        self.diffusion = DiffusionModel(config)
        # Load camera calibration from JSON
        with open(self.config.calibration_json, 'r') as f:
            calibration_data = json.load(f)
        self.calibration_data = calibration_data

        if self.config.enable_goal_conditioning:
            hl_config = HighLevelConfig(
                model_type=self.config.hl_model_type,
                run_id=self.config.hl_run_id,
                entity=self.config.hl_entity,
                project=self.config.hl_project,
                checkpoint_type=self.config.hl_checkpoint_type,
                max_depth=self.config.hl_max_depth,
                num_points=self.config.hl_num_points,
                in_channels=self.config.hl_in_channels,
                use_gripper_pcd=self.config.hl_use_gripper_pcd,
                use_text_embedding=self.config.hl_use_text_embedding,
                use_dual_head=self.config.hl_use_dual_head,
                use_rgb=self.config.hl_use_rgb,
                use_gemini=self.config.hl_use_gemini,
                is_gmm=self.config.hl_is_gmm,
                dino_model=self.config.hl_dino_model,
                calibration_data=self.calibration_data,
                use_fourier_pe=self.config.hl_use_fourier_pe,
                fourier_num_frequencies=self.config.hl_fourier_num_frequencies,
                fourier_include_input=self.config.hl_fourier_include_input,
                num_transformer_layers=self.config.hl_num_transformer_layers,
                dropout=self.config.hl_dropout,
                use_source_token=self.config.hl_use_source_token,
                use_gripper_token=self.config.hl_use_gripper_token,
            )
            self.high_level = HighLevelWrapper(hl_config)

        self.renderer = None
        self.phantomize = self.config.phantomize
        self.downsample_factor = self.config.phantom_downsample_factor
        self.reset()

    def get_optim_params(self) -> dict:
        return self.diffusion.parameters()

    def reset(self):
        """Clear observation and action queues. Should be called on `env.reset()`"""
        self._queues = {
            self.obs_key: deque(maxlen=self.config.n_obs_steps),
            self.act_key: deque(maxlen=self.config.n_action_steps),
        }
        if self.config.image_features:
            self._queues["observation.images"] = deque(maxlen=self.config.n_obs_steps)
        if self.config.env_state_feature:
            self._queues["observation.environment_state"] = deque(maxlen=self.config.n_obs_steps)

    @torch.no_grad
    def select_action(self, batch: dict[str, Tensor]) -> Tensor:
        """Select a single action given environment observations.

        This method handles caching a history of observations and an action trajectory generated by the
        underlying diffusion model. Here's how it works:
          - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is
            copied `n_obs_steps` times to fill the cache).
          - The diffusion model generates `horizon` steps worth of actions.
          - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.
        Schematically this looks like:
            ----------------------------------------------------------------------------------------------
            (legend: o = n_obs_steps, h = horizon, a = n_action_steps)
            |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |
            |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |
            |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |
            |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |
            ----------------------------------------------------------------------------------------------
        Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that
        "horizon" may not the best name to describe what the variable actually means, because this period is
        actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.
        """
        state = batch['observation.state']
        current_obs = batch[self.obs_key]

        batch = self.normalize_inputs(batch)
        if self.config.use_text_embedding:
            text = batch['task']

        if self.config.image_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch["observation.images"] = torch.stack(
                [batch[key] for key in self.config.image_features], dim=-4
            )
        # Note: It's important that this happens after stacking the images into a single key.
        self._queues = populate_queues(self._queues, batch)

        if len(self._queues[self.act_key]) == 0:
            # stack n latest observations from the queue
            batch = {k: torch.stack(list(self._queues[k]), dim=1) for k in batch if k in self._queues}
            if self.config.use_text_embedding:
                batch['task'] = text

            # Save the reference observation for this action chunk (used for relative actions)
            self._relative_action_reference_eef = current_obs.squeeze(0)  # (10,)

            actions = self.diffusion.generate_actions(batch)

            # TODO(rcadene): make above methods return output dictionary?
            actions = self.unnormalize_outputs({self.act_key: actions})[self.act_key]

            self._queues[self.act_key].extend(actions.transpose(0, 1))

        action_raw = self._queues[self.act_key].popleft()
        action = self.robot_adapter.transform_action(action_raw, state, self._relative_action_reference_eef)
        action_eef = self.robot_adapter.get_eef_action(action_raw, state, self._relative_action_reference_eef)
        return action, action_eef

    def forward(self, batch: dict[str, Tensor]) -> tuple[Tensor, None]:
        """Run the batch through the model and compute the loss for training or validation."""
        if self.config.action_space == "right_eef_relative":
            batch = self.robot_adapter.compute_relative_actions(batch)

        batch = self.normalize_inputs(batch)
        if self.config.image_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch["observation.images"] = torch.stack(
                [batch[key] for key in self.config.image_features], dim=-4
            )
        batch = self.normalize_targets(batch)
        loss = self.diffusion.compute_loss(batch)
        # no output_dict so returning None
        return loss, None


def _make_noise_scheduler(name: str, **kwargs: dict) -> DDPMScheduler | DDIMScheduler:
    """
    Factory for noise scheduler instances of the requested type. All kwargs are passed
    to the scheduler.
    """
    if name == "DDPM":
        return DDPMScheduler(**kwargs)
    elif name == "DDIM":
        return DDIMScheduler(**kwargs)
    else:
        raise ValueError(f"Unsupported noise scheduler type {name}")


class DiffusionModel(nn.Module):
    def __init__(self, config: DiffusionConfig):
        super().__init__()
        self.config = config
        robot_adapter = config.get_robot_adapter()
        self.obs_key = robot_adapter.get_obs_key()
        self.act_key = robot_adapter.get_act_key()

        self.use_text_embedding = self.config.use_text_embedding
        self.use_latent_plan = self.config.use_latent_plan

        # Build observation encoders (depending on which observations are provided).
        global_cond_dim = self.config.robot_state_feature[self.obs_key].shape[0]
        if self.config.image_features:
            num_images = len(self.config.image_features)
            if self.config.use_separate_rgb_encoder_per_camera:
                encoders = [DiffusionRgbEncoder(config) for _ in range(num_images)]
                self.rgb_encoder = nn.ModuleList(encoders)
                global_cond_dim += encoders[0].feature_dim * num_images
            else:
                self.rgb_encoder = DiffusionRgbEncoder(config)
                global_cond_dim += self.rgb_encoder.feature_dim * num_images
        if self.config.env_state_feature:
            global_cond_dim += self.config.env_state_feature.shape[0]
        if self.use_text_embedding:
            self.siglip = AutoModel.from_pretrained("google/siglip-so400m-patch14-384")
            for param in self.siglip.parameters():
                param.requires_grad = False
            self.siglip_processor = AutoProcessor.from_pretrained("google/siglip-so400m-patch14-384")
            self.text_embedding_cache = {}

            self.text_proj_dim = 32
            self.text_proj = nn.Sequential(
                nn.Linear(1152, 256),
                nn.ReLU(),
                nn.Linear(256, 64),
                nn.ReLU(),
                nn.Linear(64, self.text_proj_dim)
            )
            global_cond_dim += self.text_proj_dim
        
        if self.config.use_latent_plan:
            assert self.config.hl_model_type == "mimicplay", "latent plan conditioning only supports mimicplay high-level model currently"  
            with open(self.config.calibration_json, 'r') as f:
                calibration_data = json.load(f)
            self.calibration_data = calibration_data     
                 
            hl_config = HighLevelConfig(
                model_type=self.config.hl_model_type,
                run_id=self.config.hl_run_id,
                entity=self.config.hl_entity,
                project=self.config.hl_project,
                checkpoint_type=self.config.hl_checkpoint_type,
                max_depth=self.config.hl_max_depth,
                num_points=self.config.hl_num_points,
                in_channels=self.config.hl_in_channels,
                use_gripper_pcd=self.config.hl_use_gripper_pcd,
                use_text_embedding=self.config.hl_use_text_embedding,
                use_dual_head=self.config.hl_use_dual_head,
                use_rgb=self.config.hl_use_rgb,
                use_gemini=self.config.hl_use_gemini,
                is_gmm=self.config.hl_is_gmm,
                dino_model=self.config.hl_dino_model,
                calibration_data=self.calibration_data,
                use_fourier_pe=self.config.hl_use_fourier_pe,
                fourier_num_frequencies=self.config.hl_fourier_num_frequencies,
                fourier_include_input=self.config.hl_fourier_include_input,
                num_transformer_layers=self.config.hl_num_transformer_layers,
                dropout=self.config.hl_dropout,
                use_source_token=self.config.hl_use_source_token,
                use_gripper_token=self.config.hl_use_gripper_token,
            )

            self.mimicplay_model = HighLevelWrapper(hl_config)
    
            # self.mimicplay_model = initialize_mimicplay_model(self.config.hl_entity, self.config.hl_project, self.config.hl_checkpoint_type,
            #     self.config.hl_run_id, self.config.hl_dino_model, self.config.use_text_embedding,
            #     self.config.hl_use_gripper_token, self.config.hl_use_source_token, self.config.hl_use_fourier_pe,
            #     self.config.hl_fourier_num_frequencies, self.config.hl_fourier_include_input,
            #     self.config.hl_num_transformer_layers, self.config.hl_dropout, self.config.device
            # )
                                                        
            self.latent_proj = nn.Sequential(
                nn.Linear(896, 256),
                nn.ReLU(),
                nn.Linear(256, self.config.latent_plan_dim)
            )
            
            global_cond_dim += self.config.latent_plan_dim

        self.unet = DiffusionConditionalUnet1d(config, global_cond_dim=global_cond_dim * config.n_obs_steps)

        self.noise_scheduler = _make_noise_scheduler(
            config.noise_scheduler_type,
            num_train_timesteps=config.num_train_timesteps,
            beta_start=config.beta_start,
            beta_end=config.beta_end,
            beta_schedule=config.beta_schedule,
            clip_sample=config.clip_sample,
            clip_sample_range=config.clip_sample_range,
            prediction_type=config.prediction_type,
        )

        if config.num_inference_steps is None:
            self.num_inference_steps = config.num_train_timesteps
        else:
            self.num_inference_steps = config.num_inference_steps

    # ========= inference  ============
    def conditional_sample(
        self, batch_size: int, global_cond: Tensor | None = None, generator: torch.Generator | None = None
    ) -> Tensor:
        device = get_device_from_parameters(self)
        dtype = get_dtype_from_parameters(self)

        # Sample prior.
        sample = torch.randn(
            size=(batch_size, self.config.horizon, self.config.action_feature[self.act_key].shape[0]),
            dtype=dtype,
            device=device,
            generator=generator,
        )

        for t in self.noise_scheduler.timesteps:
            # Predict model output.
            model_output = self.unet(
                sample,
                torch.full(sample.shape[:1], t, dtype=torch.long, device=sample.device),
                global_cond=global_cond,
            )
            # Compute previous image: x_t -> x_t-1
            sample = self.noise_scheduler.step(model_output, t, sample, generator=generator).prev_sample

        return sample

    def _encode_images_train(self, batch: dict[str, Tensor], batch_size: int, n_obs_steps: int) -> Tensor:
        """Encode images during training with synchronized random crops per camera.

        Groups images by camera and applies the same random crop to all images from the same camera.
        """
        camera_groups = {}
        image_key_list = list(self.config.image_features.keys())

        for idx, key in enumerate(image_key_list):
            camera_name = key.split('.')[-2]
            if camera_name not in camera_groups:
                camera_groups[camera_name] = []
            camera_groups[camera_name].append({'key': key, 'idx': idx})

        img_features_list = [None] * len(image_key_list)

        for camera_name, image_infos in camera_groups.items():
            # Compute crop params once per camera for synchronized cropping
            if self.config.crop_shape is not None:
                sample_key = image_infos[0]['key']
                img_shape = batch[sample_key].shape[2:]  # (C, H, W)
                ref_encoder = self.rgb_encoder[0] if self.config.use_separate_rgb_encoder_per_camera else self.rgb_encoder
                crop_params = ref_encoder.compute_crop_params(img_shape)
            else:
                crop_params = None

            # Encode all images from this camera with the same crop params
            for info in image_infos:
                key = info['key']
                idx = info['idx']
                images = einops.rearrange(batch[key], "b s ... -> (b s) ...")

                if self.config.use_separate_rgb_encoder_per_camera:
                    encoder = self.rgb_encoder[idx]
                else:
                    encoder = self.rgb_encoder

                img_feat = encoder(images, crop_params=crop_params)
                img_feat = einops.rearrange(img_feat, "(b s) ... -> b s ...", b=batch_size, s=n_obs_steps)
                img_features_list[idx] = img_feat

        return torch.cat(img_features_list, dim=-1)

    def _encode_images_inference(self, batch: dict[str, Tensor], batch_size: int, n_obs_steps: int) -> Tensor:
        """Encode images during inference using center crop.

        Images are stacked in batch["observation.images"] with shape (B, n_obs_steps, num_cameras, C, H, W).
        """
        num_cameras = len(self.config.image_features)

        if self.config.use_separate_rgb_encoder_per_camera:
            img_features_list = []
            for cam_idx in range(num_cameras):
                cam_images = batch["observation.images"][:, :, cam_idx]  # (B, s, C, H, W)
                cam_images = einops.rearrange(cam_images, "b s ... -> (b s) ...")
                encoder = self.rgb_encoder[cam_idx]
                img_feat = encoder(cam_images, crop_params=None)
                img_feat = einops.rearrange(img_feat, "(b s) ... -> b s ...", b=batch_size, s=n_obs_steps)
                img_features_list.append(img_feat)
            return torch.cat(img_features_list, dim=-1)
        else:
            images = batch["observation.images"]
            images = einops.rearrange(images, "b s n ... -> (b s n) ...")
            img_features = self.rgb_encoder(images, crop_params=None)
            return einops.rearrange(
                img_features, "(b s n) ... -> b s (n ...)", b=batch_size, s=n_obs_steps, n=num_cameras
            )

    def _prepare_global_conditioning(self, batch: dict[str, Tensor]) -> Tensor:
        """Encode image features and concatenate them all together along with the state vector."""
        batch_size, n_obs_steps = batch[self.obs_key].shape[:2]
        global_cond_feats = [batch[self.obs_key]]

        if self.config.image_features:
            if self.training:
                img_features = self._encode_images_train(batch, batch_size, n_obs_steps)
            else:
                img_features = self._encode_images_inference(batch, batch_size, n_obs_steps)
            global_cond_feats.append(img_features)

        if self.config.env_state_feature:
            global_cond_feats.append(batch[OBS_ENV])

        if self.use_text_embedding:
            def get_cached_embedding(instruction):
                if instruction not in self.text_embedding_cache:
                    self.text_embedding_cache[instruction] = get_siglip_text_embedding(instruction, self.siglip, self.siglip_processor)
                return self.text_embedding_cache[instruction]

            text_feats = [get_cached_embedding(i) for i in batch['task']]
            text_feats = torch.from_numpy(np.vstack(text_feats)).to(global_cond_feats[0].device)  # (B, text_dim)
            text_feats = self.text_proj(text_feats)  # (B, 32)
            text_feats = text_feats.unsqueeze(1).repeat(1, n_obs_steps, 1)
            global_cond_feats.append(text_feats)
        
        if self.use_latent_plan:

            tasks = batch["task"]* n_obs_steps
            states = einops.rearrange(batch["observation.state"], "b s ... -> (b s) ...")
            latent_plans = []

            image_key_list = list(self.config.image_features.keys())

            cam_names = []
            for key in image_key_list:
                cam = key.split(".")[-2]
                if "wrist" in key or cam in cam_names:
                    continue
                cam_names.append(cam)

            rgbs = {}
            depths = {}
            for cam in cam_names:
                rgb_key = f"observation.images.{cam}.color"
                depth_key = f"observation.images.{cam}.transformed_depth"

                if rgb_key not in batch or depth_key not in batch:
                    raise ValueError(f"Expected both {rgb_key} and {depth_key} in the batch for latent plan conditioning.")

                # (B, S, C, H, W) -> (B*S, C, H, W)
                rgbs[cam] = einops.rearrange(batch[rgb_key], "b s c h w -> (b s) c h w")
                depths[cam] = einops.rearrange(batch[depth_key], "b s c h w -> (b s) c h w")
        

            for i in range(batch_size * n_obs_steps):
                camera_obs = {}

                for cam in cam_names:
                    rgb = (rgbs[cam][i].permute(1, 2, 0).detach().cpu().numpy() * 255.0).astype(np.uint8) #H W C
                    depth = (depths[cam][i].permute(1, 2, 0).detach().cpu().numpy()* 1000.0).astype(np.uint16)  # Convert meters to mm
                    camera_obs[cam] = {"rgb": rgb, "depth": depth}

                latent_plan, _ = self.mimicplay_model.predict(
                    tasks[i], camera_obs,
                    robot_type=self.config.robot_type,
                    robot_kwargs={"observation.state": states[i].detach().cpu().numpy()},
                )  # (1, 896)
                latent_plans.append(latent_plan)
            latent_plans = torch.cat(latent_plans, axis=0)  # (B*S, 896)
            latent_plans = self.latent_proj(latent_plans)  
            latent_plans = einops.rearrange(
                latent_plans, "(b s) ... -> b s ...", b=batch_size, s=n_obs_steps   
            )# (B, S, latent_plan_dim)
            global_cond_feats.append(latent_plans)

        # Concatenate features then flatten to (B, global_cond_dim).
        return torch.cat(global_cond_feats, dim=-1).flatten(start_dim=1)

    def generate_actions(self, batch: dict[str, Tensor]) -> Tensor:
        """
        This function expects `batch` to have:
        {
            "observation.state": (B, n_obs_steps, state_dim)

            "observation.images": (B, n_obs_steps, num_cameras, C, H, W)
                AND/OR
            "observation.environment_state": (B, environment_dim)
        }
        """
        batch_size, n_obs_steps = batch[self.obs_key].shape[:2]
        assert n_obs_steps == self.config.n_obs_steps

        # Encode image features and concatenate them all together along with the state vector.
        global_cond = self._prepare_global_conditioning(batch)  # (B, global_cond_dim)

        # run sampling
        actions = self.conditional_sample(batch_size, global_cond=global_cond)

        # Extract `n_action_steps` steps worth of actions (from the current observation).
        start = n_obs_steps - 1
        end = start + self.config.n_action_steps
        actions = actions[:, start:end]

        return actions

    def compute_loss(self, batch: dict[str, Tensor]) -> Tensor:
        """
        This function expects `batch` to have (at least):
        {
            "observation.state": (B, n_obs_steps, state_dim)

            "observation.images": (B, n_obs_steps, num_cameras, C, H, W)
                AND/OR
            "observation.environment_state": (B, environment_dim)

            "action": (B, horizon, action_dim)
            "action_is_pad": (B, horizon)
        }
        """
        # Input validation.
        assert set(batch).issuperset({"observation.state", "action", "action_is_pad"})
        assert "observation.images" in batch or "observation.environment_state" in batch
        n_obs_steps = batch[self.obs_key].shape[1]
        horizon = batch[self.act_key].shape[1]
        assert horizon == self.config.horizon
        assert n_obs_steps == self.config.n_obs_steps

        # Encode image features and concatenate them all together along with the state vector.
        global_cond = self._prepare_global_conditioning(batch)  # (B, global_cond_dim)

        # Forward diffusion.
        trajectory = batch[self.act_key]
        # Sample noise to add to the trajectory.
        eps = torch.randn(trajectory.shape, device=trajectory.device)
        # Sample a random noising timestep for each item in the batch.
        timesteps = torch.randint(
            low=0,
            high=self.noise_scheduler.config.num_train_timesteps,
            size=(trajectory.shape[0],),
            device=trajectory.device,
        ).long()
        # Add noise to the clean trajectories according to the noise magnitude at each timestep.
        noisy_trajectory = self.noise_scheduler.add_noise(trajectory, eps, timesteps)

        # Run the denoising network (that might denoise the trajectory, or attempt to predict the noise).
        pred = self.unet(noisy_trajectory, timesteps, global_cond=global_cond)

        # Compute the loss.
        # The target is either the original trajectory, or the noise.
        if self.config.prediction_type == "epsilon":
            target = eps
        elif self.config.prediction_type == "sample":
            target = batch[self.act_key]
        else:
            raise ValueError(f"Unsupported prediction type {self.config.prediction_type}")

        loss = F.mse_loss(pred, target, reduction="none")

        # Mask loss wherever the action is padded with copies (edges of the dataset trajectory).
        if self.config.do_mask_loss_for_padding:
            if "action_is_pad" not in batch:
                raise ValueError(
                    "You need to provide 'action_is_pad' in the batch when "
                    f"{self.config.do_mask_loss_for_padding=}."
                )
            in_episode_bound = ~batch["action_is_pad"]
            loss = loss * in_episode_bound.unsqueeze(-1)

        return loss.mean()


class SpatialSoftmax(nn.Module):
    """
    Spatial Soft Argmax operation described in "Deep Spatial Autoencoders for Visuomotor Learning" by Finn et al.
    (https://arxiv.org/pdf/1509.06113). A minimal port of the robomimic implementation.

    At a high level, this takes 2D feature maps (from a convnet/ViT) and returns the "center of mass"
    of activations of each channel, i.e., keypoints in the image space for the policy to focus on.

    Example: take feature maps of size (512x10x12). We generate a grid of normalized coordinates (10x12x2):
    -----------------------------------------------------
    | (-1., -1.)   | (-0.82, -1.)   | ... | (1., -1.)   |
    | (-1., -0.78) | (-0.82, -0.78) | ... | (1., -0.78) |
    | ...          | ...            | ... | ...         |
    | (-1., 1.)    | (-0.82, 1.)    | ... | (1., 1.)    |
    -----------------------------------------------------
    This is achieved by applying channel-wise softmax over the activations (512x120) and computing the dot
    product with the coordinates (120x2) to get expected points of maximal activation (512x2).

    The example above results in 512 keypoints (corresponding to the 512 input channels). We can optionally
    provide num_kp != None to control the number of keypoints. This is achieved by a first applying a learnable
    linear mapping (in_channels, H, W) -> (num_kp, H, W).
    """

    def __init__(self, input_shape, num_kp=None):
        """
        Args:
            input_shape (list): (C, H, W) input feature map shape.
            num_kp (int): number of keypoints in output. If None, output will have the same number of channels as input.
        """
        super().__init__()

        assert len(input_shape) == 3
        self._in_c, self._in_h, self._in_w = input_shape

        if num_kp is not None:
            self.nets = torch.nn.Conv2d(self._in_c, num_kp, kernel_size=1)
            self._out_c = num_kp
        else:
            self.nets = None
            self._out_c = self._in_c

        # we could use torch.linspace directly but that seems to behave slightly differently than numpy
        # and causes a small degradation in pc_success of pre-trained models.
        pos_x, pos_y = np.meshgrid(np.linspace(-1.0, 1.0, self._in_w), np.linspace(-1.0, 1.0, self._in_h))
        pos_x = torch.from_numpy(pos_x.reshape(self._in_h * self._in_w, 1)).float()
        pos_y = torch.from_numpy(pos_y.reshape(self._in_h * self._in_w, 1)).float()
        # register as buffer so it's moved to the correct device.
        self.register_buffer("pos_grid", torch.cat([pos_x, pos_y], dim=1))

    def forward(self, features: Tensor) -> Tensor:
        """
        Args:
            features: (B, C, H, W) input feature maps.
        Returns:
            (B, K, 2) image-space coordinates of keypoints.
        """
        if self.nets is not None:
            features = self.nets(features)

        # [B, K, H, W] -> [B * K, H * W] where K is number of keypoints
        features = features.reshape(-1, self._in_h * self._in_w)
        # 2d softmax normalization
        attention = F.softmax(features, dim=-1)
        # [B * K, H * W] x [H * W, 2] -> [B * K, 2] for spatial coordinate mean in x and y dimensions
        expected_xy = attention @ self.pos_grid
        # reshape to [B, K, 2]
        feature_keypoints = expected_xy.view(-1, self._out_c, 2)

        return feature_keypoints


class DiffusionRgbEncoder(nn.Module):
    """Encodes an RGB image into a 1D feature vector.

    Includes the ability to normalize and crop the image first.
    """

    def __init__(self, config: DiffusionConfig):
        super().__init__()
        # Set up optional preprocessing.
        self.crop_shape = config.crop_shape
        self.crop_jitter = config.crop_jitter
        if config.crop_shape is not None:
            self.do_crop = True
            # Always use center crop for eval
            self.center_crop = torchvision.transforms.CenterCrop(config.crop_shape)
        else:
            self.do_crop = False

        # Set up backbone.
        backbone_model = getattr(torchvision.models, config.vision_backbone)(
            weights=config.pretrained_backbone_weights
        )
        # Note: This assumes that the layer4 feature map is children()[-3]
        # TODO(alexander-soare): Use a safer alternative.
        self.backbone = nn.Sequential(*(list(backbone_model.children())[:-2]))
        if config.use_group_norm:
            if config.pretrained_backbone_weights:
                raise ValueError(
                    "You can't replace BatchNorm in a pretrained model without ruining the weights!"
                )
            self.backbone = _replace_submodules(
                root_module=self.backbone,
                predicate=lambda x: isinstance(x, nn.BatchNorm2d),
                func=lambda x: nn.GroupNorm(num_groups=x.num_features // 16, num_channels=x.num_features),
            )

        # Set up pooling and final layers.
        # Use a dry run to get the feature map shape.
        # The dummy input should take the number of image channels from `config.image_features` and it should
        # use the height and width from `config.crop_shape` if it is provided, otherwise it should use the
        # height and width from `config.image_features`.

        # Note: we have a check in the config class to make sure all images have the same shape.
        images_shape = next(iter(config.image_features.values())).shape
        dummy_shape_h_w = config.crop_shape if config.crop_shape is not None else images_shape[1:]
        dummy_shape = (1, images_shape[0], *dummy_shape_h_w)
        feature_map_shape = get_output_shape(self.backbone, dummy_shape)[1:]

        self.pool = SpatialSoftmax(feature_map_shape, num_kp=config.spatial_softmax_num_keypoints)
        self.feature_dim = config.spatial_softmax_num_keypoints * 2
        self.out = nn.Linear(config.spatial_softmax_num_keypoints * 2, self.feature_dim)
        self.relu = nn.ReLU()

    def forward(self, x: Tensor, crop_params: dict | None = None) -> Tensor:
        """
        Args:
            x: (B, C, H, W) image tensor with pixel values in [0, 1].
            crop_params: Optional dict with 'top' and 'left' keys for synchronized cropping.
                        If None and training, random crop will be computed.
        Returns:
            (B, D) image feature.
        """
        # Preprocess: maybe crop (if it was set up in the __init__).
        if self.do_crop:
            if crop_params is not None:
                # Use provided crop parameters for synchronized cropping
                x = self._apply_crop(x, crop_params['top'], crop_params['left'])
            elif self.training:
                # Random crop during training (when no params provided)
                x = self._compute_and_apply_random_crop(x)
            else:
                # Always use center crop for eval.
                x = self.center_crop(x)
        # Extract backbone feature.
        x = torch.flatten(self.pool(self.backbone(x)), start_dim=1)
        # Final linear layer with non-linearity.
        x = self.relu(self.out(x))
        return x

    def _compute_and_apply_random_crop(self, x: Tensor) -> Tensor:
        """Compute random crop parameters and apply them."""
        _, _, height, width = x.shape
        crop_params = self.compute_crop_params((x.shape[1], height, width))
        return self._apply_crop(x, crop_params['top'], crop_params['left'])

    def _apply_crop(self, x: Tensor, top: int, left: int) -> Tensor:
        """Apply crop with given parameters."""
        crop_h, crop_w = self.crop_shape
        return x[:, :, top:top + crop_h, left:left + crop_w]

    def compute_crop_params(self, img_shape: tuple) -> dict:
        """Compute random crop parameters for synchronized cropping across modalities.

        Args:
            img_shape: (C, H, W) shape of the image

        Returns:
            dict with 'top' and 'left' keys
        """
        _, height, width = img_shape
        crop_h, crop_w = self.crop_shape

        # Calculate center position with random jitter
        center_y = height // 2
        center_x = width // 2

        jitter_y = torch.randint(-self.crop_jitter, self.crop_jitter + 1, (1,)).item()
        jitter_x = torch.randint(-self.crop_jitter, self.crop_jitter + 1, (1,)).item()

        # Calculate top-left corner of crop
        top = center_y + jitter_y - crop_h // 2
        left = center_x + jitter_x - crop_w // 2

        # Clamp to ensure crop stays within image bounds
        top = max(0, min(top, height - crop_h))
        left = max(0, min(left, width - crop_w))

        return {'top': top, 'left': left}


def _replace_submodules(
    root_module: nn.Module, predicate: Callable[[nn.Module], bool], func: Callable[[nn.Module], nn.Module]
) -> nn.Module:
    """
    Args:
        root_module: The module for which the submodules need to be replaced
        predicate: Takes a module as an argument and must return True if the that module is to be replaced.
        func: Takes a module as an argument and returns a new module to replace it with.
    Returns:
        The root module with its submodules replaced.
    """
    if predicate(root_module):
        return func(root_module)

    replace_list = [k.split(".") for k, m in root_module.named_modules(remove_duplicate=True) if predicate(m)]
    for *parents, k in replace_list:
        parent_module = root_module
        if len(parents) > 0:
            parent_module = root_module.get_submodule(".".join(parents))
        if isinstance(parent_module, nn.Sequential):
            src_module = parent_module[int(k)]
        else:
            src_module = getattr(parent_module, k)
        tgt_module = func(src_module)
        if isinstance(parent_module, nn.Sequential):
            parent_module[int(k)] = tgt_module
        else:
            setattr(parent_module, k, tgt_module)
    # verify that all BN are replaced
    assert not any(predicate(m) for _, m in root_module.named_modules(remove_duplicate=True))
    return root_module


class DiffusionSinusoidalPosEmb(nn.Module):
    """1D sinusoidal positional embeddings as in Attention is All You Need."""

    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: Tensor) -> Tensor:
        device = x.device
        half_dim = self.dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = x.unsqueeze(-1) * emb.unsqueeze(0)
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb


class DiffusionConv1dBlock(nn.Module):
    """Conv1d --> GroupNorm --> Mish"""

    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):
        super().__init__()

        self.block = nn.Sequential(
            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),
            nn.GroupNorm(n_groups, out_channels),
            nn.Mish(),
        )

    def forward(self, x):
        return self.block(x)


class DiffusionConditionalUnet1d(nn.Module):
    """A 1D convolutional UNet with FiLM modulation for conditioning.

    Note: this removes local conditioning as compared to the original diffusion policy code.
    """

    def __init__(self, config: DiffusionConfig, global_cond_dim: int):
        super().__init__()

        self.config = config
        robot_adapter = config.get_robot_adapter()
        self.obs_key = robot_adapter.get_obs_key()
        self.act_key = robot_adapter.get_act_key()

        # Encoder for the diffusion timestep.
        self.diffusion_step_encoder = nn.Sequential(
            DiffusionSinusoidalPosEmb(config.diffusion_step_embed_dim),
            nn.Linear(config.diffusion_step_embed_dim, config.diffusion_step_embed_dim * 4),
            nn.Mish(),
            nn.Linear(config.diffusion_step_embed_dim * 4, config.diffusion_step_embed_dim),
        )

        # The FiLM conditioning dimension.
        cond_dim = config.diffusion_step_embed_dim + global_cond_dim

        # In channels / out channels for each downsampling block in the Unet's encoder. For the decoder, we
        # just reverse these.
        in_out = [(config.action_feature[self.act_key].shape[0], config.down_dims[0])] + list(
            zip(config.down_dims[:-1], config.down_dims[1:], strict=True)
        )

        # Unet encoder.
        common_res_block_kwargs = {
            "cond_dim": cond_dim,
            "kernel_size": config.kernel_size,
            "n_groups": config.n_groups,
            "use_film_scale_modulation": config.use_film_scale_modulation,
        }
        self.down_modules = nn.ModuleList([])
        for ind, (dim_in, dim_out) in enumerate(in_out):
            is_last = ind >= (len(in_out) - 1)
            self.down_modules.append(
                nn.ModuleList(
                    [
                        DiffusionConditionalResidualBlock1d(dim_in, dim_out, **common_res_block_kwargs),
                        DiffusionConditionalResidualBlock1d(dim_out, dim_out, **common_res_block_kwargs),
                        # Downsample as long as it is not the last block.
                        nn.Conv1d(dim_out, dim_out, 3, 2, 1) if not is_last else nn.Identity(),
                    ]
                )
            )

        # Processing in the middle of the auto-encoder.
        self.mid_modules = nn.ModuleList(
            [
                DiffusionConditionalResidualBlock1d(
                    config.down_dims[-1], config.down_dims[-1], **common_res_block_kwargs
                ),
                DiffusionConditionalResidualBlock1d(
                    config.down_dims[-1], config.down_dims[-1], **common_res_block_kwargs
                ),
            ]
        )

        # Unet decoder.
        self.up_modules = nn.ModuleList([])
        for ind, (dim_out, dim_in) in enumerate(reversed(in_out[1:])):
            is_last = ind >= (len(in_out) - 1)
            self.up_modules.append(
                nn.ModuleList(
                    [
                        # dim_in * 2, because it takes the encoder's skip connection as well
                        DiffusionConditionalResidualBlock1d(dim_in * 2, dim_out, **common_res_block_kwargs),
                        DiffusionConditionalResidualBlock1d(dim_out, dim_out, **common_res_block_kwargs),
                        # Upsample as long as it is not the last block.
                        nn.ConvTranspose1d(dim_out, dim_out, 4, 2, 1) if not is_last else nn.Identity(),
                    ]
                )
            )

        self.final_conv = nn.Sequential(
            DiffusionConv1dBlock(config.down_dims[0], config.down_dims[0], kernel_size=config.kernel_size),
            nn.Conv1d(config.down_dims[0], config.action_feature[self.act_key].shape[0], 1),
        )

    def forward(self, x: Tensor, timestep: Tensor | int, global_cond=None) -> Tensor:
        """
        Args:
            x: (B, T, input_dim) tensor for input to the Unet.
            timestep: (B,) tensor of (timestep_we_are_denoising_from - 1).
            global_cond: (B, global_cond_dim)
            output: (B, T, input_dim)
        Returns:
            (B, T, input_dim) diffusion model prediction.
        """
        # For 1D convolutions we'll need feature dimension first.
        x = einops.rearrange(x, "b t d -> b d t")

        timesteps_embed = self.diffusion_step_encoder(timestep)

        # If there is a global conditioning feature, concatenate it to the timestep embedding.
        if global_cond is not None:
            global_feature = torch.cat([timesteps_embed, global_cond], axis=-1)
        else:
            global_feature = timesteps_embed

        # Run encoder, keeping track of skip features to pass to the decoder.
        encoder_skip_features: list[Tensor] = []
        for resnet, resnet2, downsample in self.down_modules:
            x = resnet(x, global_feature)
            x = resnet2(x, global_feature)
            encoder_skip_features.append(x)
            x = downsample(x)

        for mid_module in self.mid_modules:
            x = mid_module(x, global_feature)

        # Run decoder, using the skip features from the encoder.
        for resnet, resnet2, upsample in self.up_modules:
            x = torch.cat((x, encoder_skip_features.pop()), dim=1)
            x = resnet(x, global_feature)
            x = resnet2(x, global_feature)
            x = upsample(x)

        x = self.final_conv(x)

        x = einops.rearrange(x, "b d t -> b t d")
        return x


class DiffusionConditionalResidualBlock1d(nn.Module):
    """ResNet style 1D convolutional block with FiLM modulation for conditioning."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        cond_dim: int,
        kernel_size: int = 3,
        n_groups: int = 8,
        # Set to True to do scale modulation with FiLM as well as bias modulation (defaults to False meaning
        # FiLM just modulates bias).
        use_film_scale_modulation: bool = False,
    ):
        super().__init__()

        self.use_film_scale_modulation = use_film_scale_modulation
        self.out_channels = out_channels

        self.conv1 = DiffusionConv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups)

        # FiLM modulation (https://arxiv.org/abs/1709.07871) outputs per-channel bias and (maybe) scale.
        cond_channels = out_channels * 2 if use_film_scale_modulation else out_channels
        self.cond_encoder = nn.Sequential(nn.Mish(), nn.Linear(cond_dim, cond_channels))

        self.conv2 = DiffusionConv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups)

        # A final convolution for dimension matching the residual (if needed).
        self.residual_conv = (
            nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()
        )

    def forward(self, x: Tensor, cond: Tensor) -> Tensor:
        """
        Args:
            x: (B, in_channels, T)
            cond: (B, cond_dim)
        Returns:
            (B, out_channels, T)
        """
        out = self.conv1(x)

        # Get condition embedding. Unsqueeze for broadcasting to `out`, resulting in (B, out_channels, 1).
        cond_embed = self.cond_encoder(cond).unsqueeze(-1)
        if self.use_film_scale_modulation:
            # Treat the embedding as a list of scales and biases.
            scale = cond_embed[:, : self.out_channels]
            bias = cond_embed[:, self.out_channels :]
            out = scale * out + bias
        else:
            # Treat the embedding as biases.
            out = out + cond_embed

        out = self.conv2(out)
        out = out + self.residual_conv(x)
        return out

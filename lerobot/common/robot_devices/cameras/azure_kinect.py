"""
This file contains utilities for recording frames from Azure Kinect cameras.
"""

import argparse
import concurrent.futures
import logging
import math
import shutil
import threading
import time
import traceback
from pathlib import Path
from threading import Thread
from typing import Optional, Tuple, Union, Dict, Any

import numpy as np
from PIL import Image

from lerobot.common.robot_devices.cameras.configs import AzureKinectCameraConfig
from lerobot.common.robot_devices.utils import (
    RobotDeviceAlreadyConnectedError,
    RobotDeviceNotConnectedError,
    busy_wait,
)
from lerobot.common.utils.utils import capture_timestamp_utc


def find_cameras(raise_when_empty=True, mock=False) -> list[dict]:
    """
    Find the names and the serial numbers of the Azure Kinect cameras
    connected to the computer.
    """
    if mock:
        # Mock implementation for testing
        return [{"serial_number": "000000000012", "name": "Azure Kinect 4K Camera"}]
    
    try:
        import pyk4a
        from pyk4a import Config, PyK4A
    except ImportError:
        raise ImportError(
            "pyk4a is required for Azure Kinect support. Install it with: pip install pyk4a"
        )

    cameras = []
    device_count = pyk4a.connected_device_count()
    
    for device_id in range(device_count):
        try:
            device = PyK4A(device_id=device_id)
            device.open()
            serial_number = device.serial
            device.close()
            cameras.append({
                "device_id": device_id,
                "serial_number": serial_number,
                "name": f"Azure Kinect 4K Camera",
            })
        except Exception as e:
            logging.warning(f"Failed to query device {device_id}: {e}")

    if raise_when_empty and len(cameras) == 0:
        raise OSError(
            "Not a single Azure Kinect camera was detected. Try re-plugging, or re-installing "
            "`pyk4a` and the Azure Kinect SDK, or updating the firmware."
        )

    return cameras


def save_image(img_array, device_id, frame_index, images_dir, image_type="color"):
    try:
        img = Image.fromarray(img_array)
        path = images_dir / f"camera_{device_id}_{image_type}_frame_{frame_index:06d}.png"
        path.parent.mkdir(parents=True, exist_ok=True)
        img.save(str(path), quality=100)
        logging.info(f"Saved image: {path}")
    except Exception as e:
        logging.error(f"Failed to save {image_type} image for camera {device_id} frame {frame_index}: {e}")


def save_images_from_cameras(
    images_dir: Path,
    device_ids: list[int] | None = None,
    fps=None,
    width=None,
    height=None,
    record_time_s=2,
    use_depth=True,
    use_ir=False,
    use_transformed_depth=False,
    use_point_cloud=False,
    use_transformed_color=False,
    mock=False,
):
    """
    Initializes all the cameras and saves images to the directory. Useful to visually identify the camera
    associated to a given device ID.
    """
    if device_ids is None or len(device_ids) == 0:
        camera_infos = find_cameras(mock=mock)
        device_ids = [cam["device_id"] for cam in camera_infos]

    if mock:
        import tests.cameras.mock_cv2 as cv2
    else:
        import cv2

    print("Connecting cameras")
    cameras = []
    for device_id in device_ids:
        print(f"{device_id=}")
        config = AzureKinectCameraConfig(
            device_id=device_id, 
            fps=fps, 
            width=width, 
            height=height, 
            use_depth=use_depth,
            use_ir=use_ir,
            use_transformed_depth=use_transformed_depth,
            use_point_cloud=use_point_cloud,
            use_transformed_color=use_transformed_color,
            mock=mock
        )
        camera = AzureKinectCamera(config)
        camera.connect()
        print(
            f"AzureKinectCamera({camera.device_id}, fps={camera.fps}, width={camera.capture_width}, "
            f"height={camera.capture_height}, color_mode={camera.color_mode}, use_depth={camera.use_depth})"
        )
        cameras.append(camera)

    images_dir = Path(images_dir)
    if images_dir.exists():
        shutil.rmtree(images_dir)
    images_dir.mkdir(parents=True, exist_ok=True)

    print(f"Saving images to {images_dir}")
    frame_index = 0
    start_time = time.perf_counter()
    
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            while True:
                now = time.perf_counter()

                for camera in cameras:
                    result = camera.read()
                    
                    if result is not None:
                        # Handle both dict and single array returns
                        if isinstance(result, dict):
                            for data_type, data in result.items():
                                if data is not None:
                                    if data_type == "color":
                                        executor.submit(save_image, data, camera.device_id, frame_index, images_dir, "color")
                                    elif data_type == "depth":
                                        # Convert depth to 8-bit for visualization
                                        depth_8bit = (data / 65535.0 * 255).astype(np.uint8)
                                        executor.submit(save_image, depth_8bit, camera.device_id, frame_index, images_dir, "depth")
                                    elif data_type == "ir":
                                        # Convert IR to 8-bit for visualization
                                        ir_8bit = (data / 65535.0 * 255).astype(np.uint8)
                                        executor.submit(save_image, ir_8bit, camera.device_id, frame_index, images_dir, "ir")
                                    elif data_type == "transformed_depth":
                                        # Convert transformed depth to 8-bit for visualization
                                        trans_depth_8bit = (data / 65535.0 * 255).astype(np.uint8)
                                        executor.submit(save_image, trans_depth_8bit, camera.device_id, frame_index, images_dir, "transformed_depth")
                                    elif data_type == "point_cloud":
                                        # Save point cloud as a .npy file instead of image
                                        pc_path = images_dir / f"camera_{camera.device_id}_point_cloud_frame_{frame_index:06d}.npy"
                                        pc_path.parent.mkdir(parents=True, exist_ok=True)
                                        np.save(str(pc_path), data)
                                    elif data_type == "transformed_color":
                                        executor.submit(save_image, data, camera.device_id, frame_index, images_dir, "transformed_color")
                        else:
                            # Single color image
                            executor.submit(save_image, result, camera.device_id, frame_index, images_dir, "color")

                if fps is not None:
                    dt_s = time.perf_counter() - now
                    busy_wait(1 / fps - dt_s)

                if time.perf_counter() - start_time > record_time_s:
                    break

                print(f"Frame: {frame_index:04d}\tLatency (ms): {(time.perf_counter() - now) * 1000:.2f}")
                frame_index += 1
                
    finally:
        print(f"Images have been saved to {images_dir}")
        for camera in cameras:
            camera.disconnect()


class AzureKinectCamera:
    """
    The AzureKinectCamera class provides an interface for Azure Kinect cameras with features:
    - Uses device ID for camera identification
    - Supports color, depth, IR, transformed depth (RGB-aligned), point clouds, and transformed color
    - Compatible with the LeRobot camera interface pattern
    
    To find available cameras, run:
    ```bash
    python lerobot/common/robot_devices/cameras/azurekinect.py --images-dir outputs/images_from_kinect_cameras
    ```

    Basic usage (color only):
    ```python
    from lerobot.common.robot_devices.cameras.configs import AzureKinectCameraConfig

    config = AzureKinectCameraConfig(device_id=0)
    camera = AzureKinectCamera(config)
    camera.connect()
    color_image = camera.read()  # Returns (H, W, 3) numpy array
    camera.disconnect()
    ```

    Advanced usage (multiple data types):
    ```python
    config = AzureKinectCameraConfig(
        device_id=0, 
        use_depth=True, 
        use_transformed_depth=True,
        use_point_cloud=True
    )
    camera = AzureKinectCamera(config)
    camera.connect()
    
    result = camera.read()  # Returns dict with keys: 'color', 'depth', 'transformed_depth', 'point_cloud'
    color_image = result['color']              # (H, W, 3) uint8
    depth_map = result['depth']                # (H, W) uint16 in mm
    aligned_depth = result['transformed_depth'] # (H, W) uint16 aligned to color camera
    point_cloud = result['point_cloud']        # (H, W, 3) float32 in meters (X, Y, Z)
    
    camera.disconnect()
    ```
    """

    def __init__(self, config: AzureKinectCameraConfig):
        self.config = config
        self.device_id = config.device_id
        
        # Store the raw (capture) resolution from the config
        self.capture_width = config.width
        self.capture_height = config.height

        # If rotated by Â±90, swap width and height
        if config.rotation in [-90, 90]:
            self.width = config.height
            self.height = config.width
        else:
            self.width = config.width
            self.height = config.height

        self.fps = config.fps
        self.channels = config.channels
        self.color_mode = config.color_mode
        self.use_depth = config.use_depth
        self.use_ir = config.use_ir
        self.use_transformed_depth = config.use_transformed_depth
        self.use_point_cloud = config.use_point_cloud
        self.use_transformed_color = config.use_transformed_color
        self.mock = config.mock

        self.camera = None
        self.capture = None
        self.is_connected = False
        self.thread = None
        self.stop_event = None
        self.color_image = None
        self.depth_map = None
        self.ir_image = None
        self.transformed_depth = None
        self.point_cloud = None
        self.transformed_color = None
        self.logs = {}

        if self.mock:
            import tests.cameras.mock_cv2 as cv2
        else:
            import cv2

        self.rotation = None
        if config.rotation == -90:
            self.rotation = cv2.ROTATE_90_COUNTERCLOCKWISE
        elif config.rotation == 90:
            self.rotation = cv2.ROTATE_90_CLOCKWISE
        elif config.rotation == 180:
            self.rotation = cv2.ROTATE_180

    def connect(self):
        if self.is_connected:
            raise RobotDeviceAlreadyConnectedError(
                f"AzureKinectCamera({self.device_id}) is already connected."
            )

        if self.mock:
            # Mock connection for testing
            self.fps = self.fps or 30
            self.capture_width = self.capture_width or 1280
            self.capture_height = self.capture_height or 720
            self.is_connected = True
            return

        try:
            import pyk4a
            from pyk4a import Config, PyK4A, ColorResolution, DepthMode, FPS
        except ImportError:
            raise ImportError(
                "pyk4a is required for Azure Kinect support. Install it with: pip install pyk4a"
            )

        # Map config values to pyk4a enums
        fps_map = {5: FPS.FPS_5, 15: FPS.FPS_15, 30: FPS.FPS_30}
        color_resolution_map = {
            (1280, 720): ColorResolution.RES_720P,
            (1920, 1080): ColorResolution.RES_1080P,
            (2560, 1440): ColorResolution.RES_1440P,
            (2048, 1536): ColorResolution.RES_1536P,
            (3840, 2160): ColorResolution.RES_3072P,
        }

        # Set defaults if not specified
        if self.fps is None:
            self.fps = 30
        if self.capture_width is None or self.capture_height is None:
            self.capture_width, self.capture_height = 1280, 720

        # Get pyk4a enums
        k4a_fps = fps_map.get(self.fps, FPS.FPS_30)
        k4a_color_res = color_resolution_map.get(
            (self.capture_width, self.capture_height), ColorResolution.RES_720P
        )
        
        # Enable depth if any depth-related feature is requested
        needs_depth = self.use_depth or self.use_transformed_depth or self.use_point_cloud or self.use_transformed_color
        k4a_depth_mode = DepthMode.NFOV_UNBINNED if needs_depth else DepthMode.OFF
        
        # Use BGRA32 format if we need transformed color (required by pyk4a)
        if self.use_transformed_color:
            from pyk4a import ImageFormat
            color_format = ImageFormat.COLOR_BGRA32
        else:
            color_format = None

        # Create configuration
        if color_format:
            k4a_config = Config(
                color_resolution=k4a_color_res,
                color_format=color_format,
                depth_mode=k4a_depth_mode,
                camera_fps=k4a_fps,
                synchronized_images_only=needs_depth,
            )
        else:
            k4a_config = Config(
                color_resolution=k4a_color_res,
                depth_mode=k4a_depth_mode,
                camera_fps=k4a_fps,
                synchronized_images_only=needs_depth,
            )

        try:
            # self.camera = PyK4A(config=k4a_config, device_id=self.device_id)
            self.camera = PyK4A(device_id=self.device_id)
            self.camera.open()
            self.camera.start()
            is_camera_open = True
        except Exception:
            is_camera_open = False
            traceback.print_exc()

        if not is_camera_open:
            # Verify that the provided device_id is valid
            camera_infos = find_cameras()
            available_device_ids = [cam["device_id"] for cam in camera_infos]
            if self.device_id not in available_device_ids:
                raise ValueError(
                    f"`device_id` is expected to be one of these available cameras {available_device_ids}, "
                    f"but {self.device_id} is provided instead. "
                    "To find the device ID you should use, run `python lerobot/common/robot_devices/cameras/azurekinect.py`."
                )
            raise OSError(f"Can't access AzureKinectCamera({self.device_id}).")

        # Update actual resolution based on selected mode
        resolution_map = {
            ColorResolution.RES_720P: (1280, 720),
            ColorResolution.RES_1080P: (1920, 1080),
            ColorResolution.RES_1440P: (2560, 1440),
            ColorResolution.RES_1536P: (2048, 1536),
            ColorResolution.RES_3072P: (3840, 2160),
        }
        actual_width, actual_height = resolution_map[k4a_color_res]
        
        if (self.capture_width, self.capture_height) != (actual_width, actual_height):
            logging.warning(
                f"Requested resolution {self.capture_width}x{self.capture_height} "
                f"mapped to {actual_width}x{actual_height}"
            )
        
        self.capture_width = actual_width
        self.capture_height = actual_height
        self.is_connected = True

    def read(self, temporary_color_mode: str | None = None) -> Union[np.ndarray, Dict[str, np.ndarray]]:
        """
        Read a frame from the camera. Returns different data based on configuration:
        
        - If only color is requested: returns np.ndarray (H, W, 3)
        - If multiple data types requested: returns Dict[str, np.ndarray] with keys:
          - 'color': (H, W, 3) uint8 array
          - 'depth': (H, W) uint16 array in millimeters  
          - 'ir': (H, W) uint16 array
          - 'transformed_depth': (H, W) uint16 array aligned to color camera
          - 'point_cloud': (H, W, 3) float32 array in meters (X, Y, Z)
          - 'transformed_color': (H, W, 3) uint8 array aligned to depth camera
        """
        if not self.is_connected:
            raise RobotDeviceNotConnectedError(
                f"AzureKinectCamera({self.device_id}) is not connected. Try running `camera.connect()` first."
            )

        if self.mock:
            # Mock implementation
            result = {}
            if True:  # Always include color for mock
                result['color'] = np.random.randint(0, 255, (self.capture_height, self.capture_width, 3), dtype=np.uint8)
            if self.use_depth:
                result['depth'] = np.random.randint(0, 5000, (self.capture_height, self.capture_width), dtype=np.uint16)
            if self.use_ir:
                result['ir'] = np.random.randint(0, 1000, (self.capture_height, self.capture_width), dtype=np.uint16)
            if self.use_transformed_depth:
                result['transformed_depth'] = np.random.randint(0, 5000, (self.capture_height, self.capture_width), dtype=np.uint16)
            if self.use_point_cloud:
                result['point_cloud'] = np.random.randn(self.capture_height, self.capture_width, 3).astype(np.float32)
            if self.use_transformed_color:
                result['transformed_color'] = np.random.randint(0, 255, (self.capture_height, self.capture_width, 3), dtype=np.uint8)
            
            # Return single array if only color requested, otherwise return dict
            if len(result) == 1 and 'color' in result:
                return result['color']
            return result

        import cv2

        start_time = time.perf_counter()

        try:
            self.capture = self.camera.get_capture()
        except Exception as e:
            raise OSError(f"Can't capture frame from AzureKinectCamera({self.device_id}): {e}")

        result = {}
        
        # Always get color image
        if self.capture.color is None:
            raise OSError(f"Can't capture color image from AzureKinectCamera({self.device_id}).")

        color_image = self.capture.color
        
        # Handle BGRA vs BGR format
        if color_image.shape[2] == 4:  # BGRA
            color_image = color_image[:, :, :3]  # Remove alpha channel
        
        requested_color_mode = self.color_mode if temporary_color_mode is None else temporary_color_mode
        if requested_color_mode not in ["rgb", "bgr"]:
            raise ValueError(
                f"Expected color values are 'rgb' or 'bgr', but {requested_color_mode} is provided."
            )

        # Convert color format as needed
        if requested_color_mode == "rgb":
            color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)

        h, w, _ = color_image.shape
        if h != self.capture_height or w != self.capture_width:
            raise OSError(
                f"Can't capture color image with expected height and width ({self.capture_height} x {self.capture_width}). "
                f"({h} x {w}) returned instead."
            )

        if self.rotation is not None:
            color_image = cv2.rotate(color_image, self.rotation)

        result['color'] = color_image
        self.color_image = color_image

        # Get additional data types as requested
        if self.use_depth:
            if self.capture.depth is None:
                raise OSError(f"Can't capture depth image from AzureKinectCamera({self.device_id}).")
            depth_map = self.capture.depth
            if self.rotation is not None:
                depth_map = cv2.rotate(depth_map, self.rotation)
            result['depth'] = depth_map
            self.depth_map = depth_map

        if self.use_ir:
            if self.capture.ir is None:
                raise OSError(f"Can't capture IR image from AzureKinectCamera({self.device_id}).")
            ir_image = self.capture.ir
            if self.rotation is not None:
                ir_image = cv2.rotate(ir_image, self.rotation)
            result['ir'] = ir_image
            self.ir_image = ir_image

        if self.use_transformed_depth:
            transformed_depth = self.capture.transformed_depth
            if transformed_depth is None:
                raise OSError(f"Can't capture transformed depth from AzureKinectCamera({self.device_id}).")
            if self.rotation is not None:
                transformed_depth = cv2.rotate(transformed_depth, self.rotation)
            result['transformed_depth'] = transformed_depth
            self.transformed_depth = transformed_depth

        if self.use_point_cloud:
            point_cloud = self.capture.depth_point_cloud
            if point_cloud is None:
                raise OSError(f"Can't generate point cloud from AzureKinectCamera({self.device_id}).")
            # Point cloud is (H, W, 3) with X, Y, Z in meters
            if self.rotation is not None:
                # For point clouds, we need to rotate each of the 3 channels
                for i in range(3):
                    point_cloud[:, :, i] = cv2.rotate(point_cloud[:, :, i], self.rotation)
            result['point_cloud'] = point_cloud
            self.point_cloud = point_cloud

        if self.use_transformed_color:
            transformed_color = self.capture.transformed_color
            if transformed_color is None:
                raise OSError(f"Can't capture transformed color from AzureKinectCamera({self.device_id}).")
            # Convert from BGRA to requested format
            if transformed_color.shape[2] == 4:
                transformed_color = transformed_color[:, :, :3]
            if requested_color_mode == "rgb":
                transformed_color = cv2.cvtColor(transformed_color, cv2.COLOR_BGR2RGB)
            if self.rotation is not None:
                transformed_color = cv2.rotate(transformed_color, self.rotation)
            result['transformed_color'] = transformed_color
            self.transformed_color = transformed_color

        # Log timing information
        self.logs["delta_timestamp_s"] = time.perf_counter() - start_time
        self.logs["timestamp_utc"] = capture_timestamp_utc()

        # Return single array if only color requested, otherwise return dict
        if len(result) == 1 and 'color' in result:
            return result['color']
        return result

    def read_loop(self):
        while not self.stop_event.is_set():
            try:
                result = self.read()
                # Store the result in individual properties for async access
                if isinstance(result, dict):
                    self.color_image = result.get('color')
                    self.depth_map = result.get('depth')
                    self.ir_image = result.get('ir')
                    self.transformed_depth = result.get('transformed_depth')
                    self.point_cloud = result.get('point_cloud')
                    self.transformed_color = result.get('transformed_color')
                else:
                    self.color_image = result
            except Exception as e:
                logging.error(f"Error reading in thread: {e}")

    def disconnect(self):
        if not self.is_connected:
            raise RobotDeviceNotConnectedError(
                f"AzureKinectCamera({self.device_id}) is not connected. Try running `camera.connect()` first."
            )

        if self.thread is not None and self.thread.is_alive():
            self.stop_event.set()
            self.thread.join()
            self.thread = None
            self.stop_event = None

        if not self.mock and self.camera is not None:
            self.camera.stop()
            self.camera.close()
            self.camera = None

        self.is_connected = False

    def __del__(self):
        if getattr(self, "is_connected", False):
            self.disconnect()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Save a few frames using `AzureKinectCamera` for all cameras connected to the computer, or a selected subset."
    )
    parser.add_argument(
        "--device-ids",
        type=int,
        nargs="*",
        default=None,
        help="List of device IDs used to instantiate the `AzureKinectCamera`. If not provided, find and use all available cameras.",
    )
    parser.add_argument(
        "--fps",
        type=int,
        default=30,
        help="Set the number of frames recorded per seconds for all cameras.",
    )
    parser.add_argument(
        "--width",
        type=int,
        default=1280,
        help="Set the width for all cameras.",
    )
    parser.add_argument(
        "--height",
        type=int,
        default=720,
        help="Set the height for all cameras.",
    )
    parser.add_argument(
        "--use-depth",
        action="store_true",
        help="Enable depth capture.",
    )
    parser.add_argument(
        "--use-ir",
        action="store_true", 
        help="Enable IR capture.",
    )
    parser.add_argument(
        "--use-transformed-depth",
        action="store_true",
        help="Enable transformed depth (aligned to color camera).",
    )
    parser.add_argument(
        "--use-point-cloud",
        action="store_true",
        help="Enable point cloud generation.",
    )
    parser.add_argument(
        "--use-transformed-color",
        action="store_true",
        help="Enable transformed color (aligned to depth camera).",
    )
    parser.add_argument(
        "--images-dir",
        type=Path,
        default="outputs/images_from_kinect_cameras",
        help="Set directory to save a few frames for each camera.",
    )
    parser.add_argument(
        "--record-time-s",
        type=float,
        default=2.0,
        help="Set the number of seconds used to record the frames.",
    )
    args = parser.parse_args()
    save_images_from_cameras(**vars(args))